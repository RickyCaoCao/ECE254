# ECE 254 Notes Part 3

### Lecture 19 - Memory

**Main Memory**

- CPU fetches and decodes instruction from memory, fetch operands from memory, store results back in memory
  - Simple instruction -> 4 memory accesses
- Developers assume:
  1. Main memory is unlimited
  2. All of main memory is available to program
- Most OSes manage memory

**No Memory Management**

- Up to 1980s, there was no memory management strategy
- Memory was treated like a linear array
- Programs directly accessed memory addresses and programs needed to state "start" address (location after OS and drivers) and "end" address
  - Problem: No coordination

**Solution**?

- On process switch, move all memory contents to disk
  - Problem: Very expensive (GBs of data)
  - Problem (remainder from original problem): OS is not protected and can be accessed by programs

**Solution Attempt 2 from IBM**

- Main memory divided into 2KB blocks, each with a 4-bit protection key
  - key held in special CPU registers
- Program Status Word (PSW) also contained 4-bit key
- Programs accessing memory with different 4-bit key than their PSW will result in errors
- Only OS can change protection key

**Generalized Solution 2**

- Each program gets "base" and "limit" addresses
  - Each memory access checks if `base < address_for_access < base + limit`
- Done in hardware because it's needed often and faster that way

![lect19-base_limit_check](.\Graphics\lect19-base_limit_check.PNG)

- Problem: Programs still reference absolute physical locations
  - When two processes loaded into memory from disk, then instructions might overlap (e.g `JMP` command)
  - ![lect19-abs_mem_problem](.\Graphics\lect19-abs_mem_problem.PNG)
  - Temporary Solution: Add base address of program to every address in the program when it's loaded
    - Problem: VERY SLOW
    - Problem: Hard to tell between number and address



**When to Assign Memory Location for Variable**

1. <u>Compile Time:</u> Convert variables to address locations at compile time (e.g. solution 1). Assembly and MS-DOS .COM does this.
2. <u>Load Time:</u> Addresses are updated when program is loaded (e.g. IBM solution)
   - Compiler needs to indicate what number are addresses so it can be updated
3. <u>Execution Time:</u> Do bindings at run-time
   - will require special hardware



**Address Space**

- A set of addresses that a process can use, independent of other processes' address spaces (unless shared memory)
  - abstraction layer
- For example, each process is given an area code and the instructions only need to dial the 7-digit phone number
- <u>Logical Address:</u> Address generated by CPU (the 7-digit phone number)
- <u>Physical Address:</u> actual location in memory (area-code)
- Done via hardware: <u>relocation register</u>![lect19-relocation_register](.\Graphics\lect19-relocation_register.PNG)
- This is <u>execution-time mapping</u>
- Pro: Program easily relocatable in memory (just change the relocation register value)
- Con: Less protection as no "limit" on logical and physical address
- Con: Additions are a lot slower than comparisons (due to carry propagation times)

**Swapping**

- Procedure of moving a process from memory to disk or vice-versa
- Very expensive to switch out entire process
- Modern OSes does modified form of swapping



### Lecture 20 - Dynamic Memory Allocation

**Generalized Interface**

- `void *allocate_memory (int size)` -- C equivalent of `malloc`
  - Allocate memory block of `size` bytes and return pointer of address to first byte
- `void *deallocate memory ( void *mem_block )` -- C equivalent of `free`
  - Deallocate the memory block `mem_block` 
  - Only indicates to OS that it is available for use, does not delete value
    - That's why you can `free` something and still be able to use that pointer
  - OS keeps track of every allocated block's size (how else does it know how much to deallocate?)
  - It is not possible to return a part of a block
- How to fulfill memory allocation requests?



**Case 1: Fixed Block Sizes**

- each block is a fixed size in bytes
- a program can request more than 1 block

Implementation:

1. <u>One Size for blocks</u> (e.g. 1KB)
   - Maintain linked list of available addresses
   - Allocation: grab head/tail
   - Deallocation: place deallocated address back into head/tail
2. <u>Multiple Fixed Block Sizes</u>
   - One linked list per fixed block size
   - Thus, multiple linked lists

Pros:

- Very fast for allocation ( constant time aka. O(1) )

Cons:

- <u>Internal Fragmentation:</u> unused memory that is internal to a partition
  - Some memory is wasted (e.g. program requests 1.5 blocks worth of memory so 2 blocks is allocated)
  - Bigger memory blocks -> bigger memory waste



**Case 2: Variable Block Size**

- Similar to fixed block size but have one smallest size possible (e.g. 1 KB)

Implementation:

1. <u>Bitmaps</u>

   - Given that memory has `M` units, each with `n` bits
   - Bitmap is bit array of length `M`. If the `kth` bit is 
     - 0, then the `kth` unit is unallocated
     - 1, then the `kth` unit has been allocated
   - Memory Overhead: `1/(n+1)`
     - For example, n == 4 bytes == 32 bits so overhead is `1/33 == 3%` 
     - 16 bytes == 128 bits so overhead is `1/129 == 0.8%`
   - To find a block of `k` bytes, we need to search bitmap for `8k/n` consecutive zeroes

2. <u>Doubly Linked Lists</u>

   - Each node has:
     - Start Address
     - Size of memory block
     - Allocated/Unallocated bit
   - Starts with one contiguous block of unallocated memory as only node
   - If a program requests 128 bytes, then we cut-out 128 bytes and create a new node
     - Linked list now has 2 nodes
     - Node 1: Start address, size == 128 bytes, bit == allocated
     - Node 2: Another start address, size == remaining size, bit == unallocated
   - If program deallocates memory, then bit will change from allocated to unallocated
   - <u>Coalescence:</u> Merging two or more adjacent unallocated blocks into 1 larger block
     - We need to do this because after a number of cuts, we might not have a continuous memory block large enough to support a request
     - performed periodically or when block of memory is freed
     - Check if `previous` and `next` of each free block is also free
     - Problem: Free memory blocks is spread all over main memory and can't be joined
   - <u>External Fragmentation:</u> When free memory is spread into little tiny fragments
     - External because the freed memory is not inside a memory block (as opposed to "internal fragmentation")
     - Solution 1 - Increase internal fragmentation
       - If there's extra memory in a block then requested, give it to the program anyways
       - Systems tend to round up to nearest power of 2
       - Problem: Doesn't solve external fragmentation, only decreases it
     - Solution 2 - <u>Compaction</u> (aka. <u>Relocation</u>)
       - Move all allocated memory next to one another
       - Problem: Very expensive
       - Problem: Impossible to do in some languages (like C) as we operate directly on memory addresses
     - Solution 3 - Using different allocation strategies

   

**Variable Allocation Strategies**

- Given request for size `N`, do we put it in 1st block that satisfies the condition we find or....
- Consider **two linked lists**: one for allocated and one for unallocated memory

Strategy 1 - First Fit

- Start at beginning of memory and check each block
- A block that is `> size N` is split into allocated block of size N and unallocated block of remaining size
- `O(n)` runtime, where `n` is # of blocks



Strategy 2 - Next Fit

- Similar to first fit but starts at beginning of where last block was allocated
- Prevents over-usage and concentration of external fragmentation at the start of memory
- `O(n)` runtime



Strategy 3 - Best Fit

- Consider all blocks and find the smallest block that is `> size N`
- If we check every block: `Theta(n)` runtime
- If we order blocks by increasing size: `O(n)`
  - If use AVL or red-black tree, then `Theta(ln(n))` runtime



Strategy 4 - Worst Fit

- Problem with best fit is that the "chopped" unallocated memory is probably useless
- This finds the biggest block that is `> size N` so that the "chopped" block can still be used
- Implementation: max heap or other heap



Strategy 5 - Quick Fit

- Optimization to other fitting strategies by keeping a list of blocks of fixed size (e.g. 1 MB) that are commonly requested
- ![lect20-allocation_strats](.\Graphics\lect20-allocation_strats.PNG)

**Choosing a Strategy**

1. First/Next fit
   - Fastest performance
   - Least wasted space
   - Next fit tends to do allocations at end of memory, breaking up the largest free memory block
2. Best Fit
   - Slower than first/next fit
   - Also least wasted space (space usage about the same)
3. Worst fit performs worst
   - Slowest Performance
   - Most wasted space

- Even with optimization, given `x` allocated blocks, up to `0.5x` blocks can be lost to fragmentation



**Binary Buddy**

- Memory blocks are available in powers of 2
- Some internal fragmentation, less external fragmentation

Given a memory request of size `n`,

1. Repeatedly split the size by half until we get the smallest block greater or equal to size `n`

2. In subsequent memory requests, we check:

   a. Is there already a memory block that meets the needs (smallest block `>= size n`)

   b. Can we subdivide a block to get there

3. Cleanup: A pair of buddies (2 blocks of equal size beside each other) can be coalesced if they are both free

![lect20-binary_buddy](.\Graphics\lect20-binary_buddy.PNG)

### Lecture 21 - Memory Segmentation and Paging

 <u>Segments:</u> memory 'elements' such as the stack, heap, standard C library

- Segments of a program (constructed by compiler)
  1. Code (aka instructions)
  2. Global Variables
  3. Heap
  4. Stack (one per thread)
  5. Standard C library
- Think of memory as a tuple `<segment, offset>`
  - Segment table maps tuple to pure memory addresses
  - Each entry in table has <u>base address</u> and <u>limit</u> (ie. size of segment)
- Hardware manages memory access (addition operation of base + offset) and memory verification (making sure offset < limit)

![lect21-memory_access](.\Graphics\lect21-memory_access.PNG)

**Paging**

<u>Frames:</u> Small, fixed-size memory chunks

<u>Pages:</u> A processes' memory is divided into the same size as frames

- Pages are assigned frames
- A frame may be empty or have exactly 1 page



<u>Key Benefit:</u> Frames for a process do not have to be continuous

- Separates logical address with physical address for programmers
- External fragmentation is completely eliminated
- Internal fragmentation is very small (at max, each memory request is `full_page - 1`)
- Process cannot access memory outside of memory space



<u>Downsides:</u> 

- Each process requires a page table to keep track of which frames the process uses
- List of free frames is also needed

- See Page 3 for diagram https://learn.uwaterloo.ca/d2l/le/content/391638/viewContent/2170154/View 



<u>Details:</u>

- Given logical address space with size `2^m` and page size of `2^n`, then
  - first `m-n` bytes is page number
  - last `n` bytes is page offset
- Page sizes are typically the size of disk read/write size
  - Modern systems have page size of about 4 KB 
- OS operates on the <u>frame table</u> - a listing of all the frames and the process' page the frame is holding (if any)

**Shared Pages**

- Sharing of pages allow for reduced memory consumption
  - For example, common parts of Chrome can be shared across all tabs
- <u>Reentrant</u>: Pure/Stateless
  - Any code can be shared if it is reentrant
  - Any function that accesses global / static variables are not reentrant (as they can depend on state)



**Page Table Structure**

- Page tables are large so they require structuring

1. Hierarchical Paging
   - Allows for discontinuous pages
   - Given page number `p`,
     - first `k` bits is outer page
     - second `p-k` is inner page
       - Use inner page to calculate physical address
   - Only works for 32-bit virtual addresses as the table will be too big for 64-bit
2. Hashed Page Tables
   - Hash table with each bucket implemented as a linked list
3. Inverted Page Tables
   - Create table with 1 entry per frame (so table size is size of RAM)
     - Each entry has process and its page number
   - Con: Require searching the entire table to find relation
     - However, hardware can make it faster



**Paging: Hardware Support**

- Page table kept in main memory, registers used to point to page table
- <u>Downside</u>: Need to retrieve page table and find the frame in entry, then finally access the page
  - Solution: A caching mechanism using the <u>TLB (translation lookaside buffer)</u> that maps logical addresses to physical addresses
    - <u>TLB Hit:</u> If page number is found in TLB, then frame is immediately given
    - <u>TLB Miss</u>: Retrieve page table -> find frame -> store in TLB -> access frame

![lect21-tlb](.\Graphics\lect21-tlb.PNG)

### Lecture 22 - Caching

<u>Line</u>: an entry in cache

- caches have fixed-size blocks

Cache Hit vs. Cache Miss

- Page miss is aka <u>page fault</u>

Effective Access Time:

![lect22-access_time](.\Graphics\lect22-access_time.PNG)

- hit ratio is how often we get cache hit



Cache Levels: L1, L2, L3

- From smaller -> largest, which is also fastest -> slowest
- If miss in L1 and hit in L2, then entry is copied from L2 to L1



**Page Replacement Algorithms**

- if page fault, which page entry to <u>evict</u> (aka replace) in cache

Important Observations about Memory Access

1. 10% of source code is executed 90% of the time
2. <u>Temporal Locality:</u> Recently accessed location is likely to be accessed again
3. <u>Spatial Locality:</u> Memory location nearby recently accessed location is more likely to be accessed

- If page has been altered in cache, we can delay writing to main memory when entry is evicted IF page is not shared



<u>Important</u>: Should priortize replacing a page entry in cache that has not been modified to save time writing to main memory



<u>Optimal Algorithm</u>

- The best algorithm possible is to replace the page that will be used most distantly in future
- Impossible to implement (can't see into future)
- Used as benchmark for other algorithms



<u>Not-Recently-Used</u>

- Each page has two status bits
  1. `R` == referenced bit == set when page is read or written to
  2. `M` == modified bit == set when page is written to
- `R` is periodically cleared to 0
- Ordering for Replacement
  1. Not Referenced, Not Modified
  2. Not Referenced, Modified
  3. Referenced, Not Modified
  4. Referenced, Modified

<u>FIFO</u>

- Replace oldest frame
- Have a counter that points to frame to be replaced
  - Increment by 1 to next frame to be replaced
- Downside: doesn't take into consideration how often frame is used



<u>Second Chance (Clock Algorithm)</u>

- Similar to FIFO but if page entry has `R == 1`, then clear R and continue to next oldest
- Even if all page entries have `R == 1`, then the initial oldest page will still be removed



<u>Least-Recently-Used (LRU)</u>

- Doubly linked list, in order from most-recently-used to least-recently-used
- If a page is used, move entry to back of list
- If page fault, then remove the head of list
- Theta(1) runtime



<u>Not Frequently Used (NFU)</u>

- Each page gets associated software counter
- Whenever page is referenced, add to counter
- <u>Aging:</u> A method to decline count over time
  - Counters are periodically right-shifted
  - An "add" to counter is setting the leftmost bit as `1`
- Downside: does lose history if two pages have same counter



<u>Pre-Paging</u>

- pre-populate cache by guessing pages needed using temporal and spatial locality
- useful when program is newly started or just recently swapped from memory



<u>Verdict (in order of efficiency)</u>

1. LRU (given that hardware is available to support it)
   - hard to implement
2. NFU + Aging
3. NFU
4. Everything else is relatively suboptimal



**Local and Global Algorithms**

- replace pages used by the process (local) or any pages (global)
  - smaller caches have to use global replacements for efficiency
- global algorithms == dynamic allocation == better
  - local algorithms == fixed allocation == wasted cache space
- keep track of page faults using <u>Page Fault Frequency (PFF)</u> to determine if a process has enough allocated cache space
  - Using PFF assumes that processes will have fewer page faults if given more cache
    - Works for LRU algo but not FIFO



### Lecture 23 + 24 - Virtual Memory

- physical memory may not be enough memory
  - a program that uses large datasets from server
  - sum of memory requirements from multiple running programs

Solution: Running programs partly in memory, partly on disk

- Use memory as a "cache" for disk
  - If a page is not found in main memory, we have a <u>page fault</u> and a page in main memory will be evicted by a page from disk
  - Page replacement algorithm needed
  - <u>Demand Paging</u>:  A page is loaded into memory only if it is referenced or needed

![lect23-virtual_memory_mapping](.\Graphics\lect23-virtual_memory_mapping.PNG)

- Pros:
  - Program no longer constrained by size of physical memory
  - Each program uses less physical memory, allowing more processes to run concurrently
  - Less I/O is needed to swap user programs



<u>Thrashing:</u> OS spends too much time swapping pages in/out of memory so very little actual work gets done



**6 Step Process of Memory Reference w/ Swapping:**

1. Check if memory reference is valid or invalid
   - Terminate if invalid
2. Check if page reference is in main memory. Assume reference is not in memory.
3. If page fault, find free frame in main memory (or evict some other page)
4. Disk read (or write) of new page
5. Update records to show new page is in main memory
6. Restart instruction that referenced page
   - To allow for restart of any instruction, the state of the process (eg. register values) must be saved when page fault occurs
   - <u>Problem</u>: Instructions where source and destination overlap may not be easily restarted
     - e.g. `c = ++c + 5;`
     - Solution 1: Check source, destination address before running operation
     - Solution 2: Have temporary registers hold values and store values on page fault

![lect23-memory_swapping](.\Graphics\lect23-memory_swapping.PNG)



Important Note: It is possible for page to be a WRITE

- For example, for `C = A + B` , if the page that stores `C` is not in memory, then we need to fetch page for `C` and restart entire addition operation



**Virtual Memory Performance**

- ![lect23-virtual_memory_performance](.\Graphics\lect23-virtual_memory_performance.PNG)
  - h is hit rate for cache
  - p is hit rate for main memory
  - tc is access time for cache
  - tm is access time for main memory
  - td is access time for disk
  - Since td is order of magnitudes larger than other times, `(1-p) * td` is dominating term
- For files, system usually has a "swap file", which is a partition of hard drive
  - System will swap entire file into memory as it is faster

**Handling Page Fault** - IS THIS NECESSARY???

1. Send trap to OS
2. Save user registers and process states
3. Identify interrupt as page fault
4. Verify page reference.
   - If valid, determine location of page on disk
   - If invalid, terminate program
5. Identify frame for page (using replacement algorithm)
6. Check if frame has been modified
   - If yes, write modified page to disk
     - Receive interrupt when disk write has completed
     - Resave registers as CPU was doing something else during disk write
     - Update page tables and mark frame as free
7. Issue disk read
   - CPU does something else in meantime
8. Receive interrupt when disk read is completed
   - Save registers and states if CPU was doing something else
9. Update page tables
10. Restore values and state and restart interrupted instruction



**Copy-on-Write**

- UNIX systems uses <u>copy-on-write technique</u>
- When UNIX system processes `fork()`, the pages are shared but marked with "copy-on-write"
- When either the parent or child attempts to modify a shared page, a copy of the page is made
- `vfork()` skips copy-on-write technique by immediately suspending parent process
  - Dangerous as modifications to pages done in child will be visible by parent
  - Efficient if are going to run `exec()` in parent anyways and get new memory space



**Allocation of Frames**

- designate a few frames to always be free
  - offers us the ability to save time as we don't have to write page that was occupying the frame
- create a minimum number of frames for processes 
  - e.g. if instruction and addresses were in 2 pages, then 1 frame is not enough
  - e.g. if each instruction has an indirect address, this could cause infinite nesting (aka stack overflow)
    - as a result, we need to limit the levels of indirection
    - if levels of indirection == 16, then worst case scenario requires 17 pages minimum



**Frame Allocation Algorithms**

- Assume there are `m` frames, `k` of which is used by OS. Thus, `m-k` frames available



<u>Equal Allocation</u>

- Given `n` processes, each process gets `(m-k)/n` frames
- Leftover frames can be used as pool of free frames (see above)
- <u>Downside</u>: Does not take into account how many frames the program actually needs



<u>Proportional Allocation</u>

- If each process `Pi` has a virtual memory size of `Si` , and `S` is the sum of all virtual memory sizes, then each process gets `Si/S * (m-k)` frames
- Frames need to be an integer value and above minimum, and sum of all frames must be less than `(m-k)`
- <u>Downside</u>: Does not consider priority of processes



<u>Local and Global Replacement</u>

- With a global replacement algorithm, the process the runs the most will acquire the most pages
- Requires overhead to ensure no process falls below minimum number of pages OR swap entire process to disk when pages go below minimum



**Thrashing**

- Background: basic CPUs will start and bring in more processes into memory when CPU utilization is low
- However, when a process runs into a high number of page faults, this process takes other process' pages. But then those processes can't run
- Since no processes are running, the CPU utilization is low so more programs are brought in == **fatal error**
- This example demonstrates **the CPU usage alone is a bad indicator for determining if more or fewer processes need to be running**



<u>Solution 1: Change to local replacement</u>

- Pro:  Process with page faults will not steal pages from other processes
- Con: Process with page faults is still hogging disk time

<u>Solution 2: Track number of page faults</u>

- Pro: Tells us if too many processes are in memory
- Con: Reactive solution



**Solution 3: Working-Set Model**

- Allocating enough frames for a process's locality and take advantage of temporal and spatial locality
  - The process will eventually leave locality -- unavoidable



First, the OS will determine an appropriate `n` for the process

- The last `n` most recently accessed pages are stored in the <u>working-set</u> and represent the locale of the program
  - If the last `n` accesses were all in page `k`, then the working-set will only have 1 page
- If `n` is too large, then it will cover too many locales.
  - Fewer processes will be allowed to run
- If `n` is too small, then it will not encompass the entire locale 
  - More page faults



`Sum of all working sets < (m-k)`

- If `sum > (m-k)` , the OS will know that the system is overloaded and remove a process to prevent thrashing
- If `sum << (m-k)`, the system is underloaded and more processes can start



**Page Fault Behaviour**

- Program Start: High page fault rate
- Established First Locale: Low
- Move to New Locale: High
- Settled in New Locale: Low

![lect24-memory_locales](.\Graphics\lect24-memory_locales.PNG)

